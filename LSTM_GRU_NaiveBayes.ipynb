{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Copy of LSTM (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHjiIur1Cw5j"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtcQBoE3D-Y"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras import models, layers, optimizers, datasets, utils, losses\n",
        "\n",
        "vocabulary_size = 10000\n",
        "maxlen = 40\n",
        "batch_size = 25\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=vocabulary_size)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7oyaRWSC5Gv"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u5C9mPk3D-Y",
        "outputId": "b7831efb-f390-407a-b668-8adb9d6f5ff9"
      },
      "source": [
        "inputs = layers.Input(shape=(maxlen,))\n",
        "e=layers.Embedding(vocabulary_size, 128)(inputs)\n",
        "h=layers.LSTM(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(h)\n",
        "model_LSTM = models.Model(inputs, outputs)\n",
        "\n",
        "model_LSTM.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_LSTM.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score_LSTM, acc_LSTM = model_LSTM.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test accuracy:', acc_LSTM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/4\n",
            "1000/1000 [==============================] - 93s 93ms/step - loss: 0.5240 - accuracy: 0.7276 - val_loss: 0.4212 - val_accuracy: 0.8054\n",
            "Epoch 2/4\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.4032 - accuracy: 0.8164 - val_loss: 0.4236 - val_accuracy: 0.8058\n",
            "Epoch 3/4\n",
            "1000/1000 [==============================] - 91s 91ms/step - loss: 0.3608 - accuracy: 0.8405 - val_loss: 0.4366 - val_accuracy: 0.8064\n",
            "Epoch 4/4\n",
            "1000/1000 [==============================] - 89s 89ms/step - loss: 0.3282 - accuracy: 0.8555 - val_loss: 0.4532 - val_accuracy: 0.8056\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4532 - accuracy: 0.8056\n",
            "Test accuracy: 0.8055599927902222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e_O2IGmC8Tn"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KEilOABbIId",
        "outputId": "ba49bd76-d56c-4324-d5dc-89206caf16b0"
      },
      "source": [
        "inputs = layers.Input(shape=(maxlen,))\n",
        "e=layers.Embedding(vocabulary_size, 128)(inputs)\n",
        "h=layers.GRU(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "#h=layers.LSTM(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(h)\n",
        "model_GRU = models.Model(inputs, outputs)\n",
        "\n",
        "model_GRU.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_GRU.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score_GRU, acc_GRU = model_GRU.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test accuracy:', acc_GRU)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/4\n",
            "1000/1000 [==============================] - 85s 85ms/step - loss: 0.5486 - accuracy: 0.7082 - val_loss: 0.4275 - val_accuracy: 0.8027\n",
            "Epoch 2/4\n",
            "1000/1000 [==============================] - 86s 86ms/step - loss: 0.4124 - accuracy: 0.8141 - val_loss: 0.4094 - val_accuracy: 0.8112\n",
            "Epoch 3/4\n",
            "1000/1000 [==============================] - 86s 86ms/step - loss: 0.3625 - accuracy: 0.8382 - val_loss: 0.4028 - val_accuracy: 0.8158\n",
            "Epoch 4/4\n",
            "1000/1000 [==============================] - 85s 85ms/step - loss: 0.3269 - accuracy: 0.8595 - val_loss: 0.4116 - val_accuracy: 0.8146\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.4116 - accuracy: 0.8146\n",
            "Test accuracy: 0.8146399855613708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZwTIQ4EC-RJ"
      },
      "source": [
        "#Dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbczYnv3bPr5",
        "outputId": "804d797a-bb3a-443a-a491-3089c928fd6b"
      },
      "source": [
        "inputs = layers.Input(shape=(maxlen,))\n",
        "e=layers.Embedding(vocabulary_size, 128)(inputs)\n",
        "h=layers.Dense(128)(e)\n",
        "# h=layers.GRU(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "# h=layers.LSTM(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(h)\n",
        "model_Dense1 = models.Model(inputs, outputs)\n",
        "\n",
        "model_Dense1.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_Dense1.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score_Dense1, acc_Dense1 = model_Dense1.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test accuracy:', acc_Dense1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1000/1000 [==============================] - 21s 21ms/step - loss: 0.6800 - accuracy: 0.5503 - val_loss: 0.6774 - val_accuracy: 0.5536\n",
            "Epoch 2/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6736 - accuracy: 0.5609 - val_loss: 0.6774 - val_accuracy: 0.5541\n",
            "Epoch 3/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6723 - accuracy: 0.5625 - val_loss: 0.6778 - val_accuracy: 0.5570\n",
            "Epoch 4/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6715 - accuracy: 0.5634 - val_loss: 0.6780 - val_accuracy: 0.5568\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6780 - accuracy: 0.5568\n",
            "Test accuracy: 0.556821882724762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VIZzmQDCtL"
      },
      "source": [
        "#Dense with dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW8kRgydbSxo",
        "outputId": "ac29f176-e720-490e-bbe7-dac8f79f6f9e"
      },
      "source": [
        "inputs = layers.Input(shape=(maxlen,))\n",
        "e=layers.Embedding(vocabulary_size, 128)(inputs)\n",
        "h=layers.Dense(128)(e)\n",
        "d=layers.Dropout(0.8)(h)\n",
        "# h=layers.GRU(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "# h=layers.LSTM(128, dropout=0.8, recurrent_dropout=0.8)(e)\n",
        "outputs=layers.Dense(1, activation='sigmoid')(d)\n",
        "model_Dense2 = models.Model(inputs, outputs)\n",
        "\n",
        "model_Dense2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_Dense2.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=4,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score_Dense2, acc_Dense2 = model_Dense2.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test accuracy:', acc_Dense2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6813 - accuracy: 0.5487 - val_loss: 0.6776 - val_accuracy: 0.5563\n",
            "Epoch 2/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6748 - accuracy: 0.5594 - val_loss: 0.6775 - val_accuracy: 0.5569\n",
            "Epoch 3/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6730 - accuracy: 0.5623 - val_loss: 0.6777 - val_accuracy: 0.5569\n",
            "Epoch 4/4\n",
            "1000/1000 [==============================] - 20s 20ms/step - loss: 0.6723 - accuracy: 0.5631 - val_loss: 0.6781 - val_accuracy: 0.5539\n",
            "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6781 - accuracy: 0.5539\n",
            "Test accuracy: 0.5538750290870667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ib_u5vDFsp"
      },
      "source": [
        "#Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vpjoTebdVy",
        "outputId": "a2c46633-6c4f-453f-d709-a498c24fa7cc"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "model_Naive = MultinomialNB()\n",
        "model_Naive.fit(x_train, y_train)\n",
        "acc_Naive = model_Naive.score(x_test,y_test)\n",
        "print('Test accuracy:', acc_Naive)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.50704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y62ZIp6SDP3l"
      },
      "source": [
        "#Picking three random word embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofhtmlsd8g4N"
      },
      "source": [
        "import random\n",
        "word1=embeddings[random.randint(0, embeddings.shape[0])]\n",
        "word2=embeddings[random.randint(0, embeddings.shape[0])]\n",
        "word3=embeddings[random.randint(0, embeddings.shape[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvM9AL-zDlPa"
      },
      "source": [
        "#Euclidean Distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn4rEcCQ-jfK",
        "outputId": "733d1a41-2d69-42a7-a7bd-7746b281155f"
      },
      "source": [
        "import numpy as np\n",
        "print(np.linalg.norm(word1-word2))\n",
        "print(np.linalg.norm(word1-word3))\n",
        "print(np.linalg.norm(word2-word3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44555712\n",
            "0.45807117\n",
            "0.4590523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKoONlboD7AY"
      },
      "source": [
        "* The Text Accuracy for LSTM is 80% whereas GRU is 81%. Not a significant change but they both are different versions so the accuracies differ slightly.\n",
        "* We took 10K words, this may be enough for the LSTM and GRU to push accuracies to 80% because usually they are used if we have less data to train. \n",
        "* The major reason behind this accuracy is we are mitigating the vanishing gradient problem, which leads to better training of model and gives better predictions. \n",
        "* The accuracies of Dense layer is around 55%, very low because it will be normal RNN which has vanishing gradient problem and the first layers don't train well. This Dense version is not able to memorize the words that are entered at first in time. So, the model has to decide only on last time cycles which makes it difficult. \n",
        "* The Naive Bayes accuracy is also 50% which is very low. Naive Bayes will use conditional probability so it is not able to find a perfect condition for each word in sentense whenever detected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3yP_KPA_The"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}